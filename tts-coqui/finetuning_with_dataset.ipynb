{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c639ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/j-i13a103'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0462f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6b4e8aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀\n",
    "def custom_formatter(root_path, manifest_file, **kwargs):\n",
    "    \"\"\"\n",
    "    Assumes each line as ```<filename>|<transcription>```\n",
    "    \"\"\"\n",
    "    items = []\n",
    "\n",
    "    with open(os.path.join(root_path, manifest_file), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            cols = line.strip().split(\"|\")\n",
    "            audio_file = os.path.join(root_path, \"wavs\", cols[0])\n",
    "            text = cols[1].strip()\n",
    "            speaker_name = cols[-1].strip()\n",
    "            items.append({\n",
    "                \"text\": text,\n",
    "                \"audio_file\": audio_file,\n",
    "                \"speaker_name\": speaker_name,\n",
    "                \"language\": \"ko\",\n",
    "                \"root_path\": root_path\n",
    "            })\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "febff70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.tts.datasets.formatters import register_formatter\n",
    "\n",
    "register_formatter(\"custom_formatter\", custom_formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d7ed542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "\n",
    "root_path = \"tts/MyTTSDataset/\"\n",
    "\n",
    "# Define here the dataset that you want to use for the fine-tuning on.\n",
    "dataset_config = BaseDatasetConfig(\n",
    "    formatter=\"custom_formatter\",\n",
    "    meta_file_train=\"metadata_with_emotion_tag_train.txt\",\n",
    "    meta_file_val=\"metadata_with_emotion_tag_val.txt\",\n",
    "    path=root_path,    # dataset 루트 경로\n",
    "    language=\"ko\"\n",
    ")\n",
    "\n",
    "# train_samples, eval_samples = load_tts_samples(\n",
    "#     dataset_config,\n",
    "#     eval_split=True,\n",
    "#     eval_split_size=0.1,\n",
    "#     formatter=custom_formatter    # 커스텀 formatter 함수 전달하기\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3820e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add here the configs of the datasets\n",
    "DATASETS_CONFIG_LIST = [dataset_config]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3907e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_samples), len(eval_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fe52425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/j-i13a103/tts/run/training\n"
     ]
    }
   ],
   "source": [
    "# 사용할 모델 : tts_models/multilingual/multi-dataset/xtts_v2\n",
    "from TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig\n",
    "from TTS.utils.manage import ModelManager\n",
    "\n",
    "# 로깅\n",
    "RUN_NAME = \"GPT_XTTS_v2.0_MyDataset_FT\"\n",
    "PROJECT_NAME=\"XTTS_trainer\"\n",
    "DASHBOARD_LOGGER = \"tensorboard\"\n",
    "LOGGER_URI = None\n",
    "\n",
    "OUT_PATH = os.path.join(os.path.dirname(os.path.abspath(root_path)), \"run\", \"training\")\n",
    "print(OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5259dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training 파라미터\n",
    "OPTIMIZER_WD_ONLY_ON_WEIGHTS = True\n",
    "START_WITH_EVAL = True\n",
    "BATCH_SIZE = 3\n",
    "GRAD_ACUMM_STEPS = 84\n",
    "\n",
    "# Note: we recommend that BATCH_SIZE * GRAD_ACUMM_STEPS need to be at least 252 for more efficient training.\n",
    "# You can increase/decrease BATCH_SIZE but then set GRAD_ACUMM_STEPS accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a30f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where XTTS v2.0.1 files will be downloaded\n",
    "CHECKPOINTS_OUT_PATH = os.path.join(OUT_PATH, \"XTTS_v2.0_original_model_files/\")\n",
    "\n",
    "os.makedirs(CHECKPOINTS_OUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "052c3a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/j-i13a103/tts/run/training/XTTS_v2.0_original_model_files/dvae.pth\n",
      "/home/j-i13a103/tts/run/training/XTTS_v2.0_original_model_files/mel_stats.pth\n"
     ]
    }
   ],
   "source": [
    "# DVAE files\n",
    "DVAE_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\"\n",
    "MEL_NORM_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth\"\n",
    "\n",
    "# Set the path to the downloaded files\n",
    "DVAE_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(DVAE_CHECKPOINT_LINK))\n",
    "MEL_NORM_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(MEL_NORM_LINK))\n",
    "\n",
    "print(DVAE_CHECKPOINT)\n",
    "print(MEL_NORM_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5ad940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Downloading DVAE files!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.07k/1.07k [00:00<00:00, 1.31kiB/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# download DVAE files if needed\n",
    "if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n",
    "    print(\" > Downloading DVAE files!\")\n",
    "    ModelManager._download_model_files([MEL_NORM_LINK, DVAE_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bcdb948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/j-i13a103/tts/run/training/XTTS_v2.0_original_model_files/vocab.json\n",
      "/home/j-i13a103/tts/run/training/XTTS_v2.0_original_model_files/model.pth\n"
     ]
    }
   ],
   "source": [
    "# Download XTTS v2.0 checkpoint if needed\n",
    "TOKENIZER_FILE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json\"\n",
    "XTTS_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth\"\n",
    "\n",
    "# XTTS transfer learning parameters: You we need to provide the paths of XTTS model checkpoint that you want to do the fine tuning.\n",
    "TOKENIZER_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(TOKENIZER_FILE_LINK))  # vocab.json file\n",
    "XTTS_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(XTTS_CHECKPOINT_LINK))  # model.pth file\n",
    "\n",
    "print(TOKENIZER_FILE)\n",
    "print(XTTS_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e2503f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Downloading XTTS v2.0 files!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211M/211M [00:50<00:00, 4.14MiB/s]\n",
      "361kiB [00:00, 593kiB/s]\n"
     ]
    }
   ],
   "source": [
    "# download XTTS v2.0 files if needed\n",
    "if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n",
    "    print(\" > Downloading XTTS v2.0 files!\")\n",
    "    ModelManager._download_model_files(\n",
    "        [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1091c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training sentences generations\n",
    "SPEAKER_REFERENCE = [\n",
    "    \"./tts/speaker_reference.wav\"  # speaker reference to be used in training test sentences\n",
    "]\n",
    "LANGUAGE = dataset_config.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a49175f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "from TTS.tts.configs.xtts_config import XttsAudioConfig\n",
    "\n",
    "def main():\n",
    "    # init args and config\n",
    "    model_args = GPTArgs(\n",
    "        max_conditioning_length=132300,  # 6 secs\n",
    "        min_conditioning_length=22050,  # 1 secs\n",
    "        debug_loading_failures=False,\n",
    "        max_wav_length=255995,  # ~11.6 seconds\n",
    "        max_text_length=200,\n",
    "        mel_norm_file=MEL_NORM_FILE,\n",
    "        dvae_checkpoint=DVAE_CHECKPOINT,\n",
    "        xtts_checkpoint=XTTS_CHECKPOINT,  # checkpoint path of the model that you want to fine-tune\n",
    "        tokenizer_file=TOKENIZER_FILE,\n",
    "        gpt_num_audio_tokens=1026,\n",
    "        gpt_start_audio_token=1024,\n",
    "        gpt_stop_audio_token=1025,\n",
    "        gpt_use_masking_gt_prompt_approach=True,\n",
    "        gpt_use_perceiver_resampler=True,\n",
    "    )\n",
    "    \n",
    "    # define audio config\n",
    "    audio_config = XttsAudioConfig(sample_rate=22050, dvae_sample_rate=22050, output_sample_rate=24000)\n",
    "    \n",
    "    # training parameters config\n",
    "    config = GPTTrainerConfig(\n",
    "        output_path=OUT_PATH,\n",
    "        model_args=model_args,\n",
    "        run_name=RUN_NAME,\n",
    "        project_name=PROJECT_NAME,\n",
    "        run_description=\"\"\"\n",
    "            GPT XTTS training\n",
    "            \"\"\",\n",
    "        dashboard_logger=DASHBOARD_LOGGER,\n",
    "        logger_uri=LOGGER_URI,\n",
    "        audio=audio_config,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        batch_group_size=48,\n",
    "        eval_batch_size=BATCH_SIZE,\n",
    "        num_loader_workers=8,\n",
    "        eval_split_max_size=256,\n",
    "        eval_split_size=0.1,\n",
    "        print_step=50,\n",
    "        plot_step=100,\n",
    "        log_model_step=1000,\n",
    "        save_step=10000,\n",
    "        save_n_checkpoints=1,\n",
    "        save_checkpoints=True,\n",
    "        \n",
    "        # target_loss=\"loss\",\n",
    "        print_eval=False,\n",
    "        \n",
    "        # Optimizer values like tortoise, pytorch implementation with modifications to not apply WD to non-weight parameters.\n",
    "        optimizer=\"AdamW\",\n",
    "        optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,\n",
    "        optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n",
    "        lr=5e-06,  # learning rate\n",
    "        lr_scheduler=\"MultiStepLR\",\n",
    "        \n",
    "        # it was adjusted accordly for the new step scheme\n",
    "        lr_scheduler_params={\"milestones\": [50000 * 18, 150000 * 18, 300000 * 18], \"gamma\": 0.5, \"last_epoch\": -1},\n",
    "        test_sentences=[\n",
    "            {\n",
    "                \"text\": \"이 케이크 정말 좋네요. 촉촉하고 달콤해서 입에서 사르르 녹아요.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"조용한 밤에 들리는 바람 소리는 참 정겨워요.\",\n",
    "                \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "                \"language\": LANGUAGE,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # init the model from config\n",
    "    model = GPTTrainer.init_from_config(config)\n",
    "\n",
    "    # load training samples\n",
    "    train_samples, eval_samples = load_tts_samples(\n",
    "        DATASETS_CONFIG_LIST,\n",
    "        eval_split=True,\n",
    "        formatter=custom_formatter\n",
    "    )\n",
    "\n",
    "    print(len(train_samples), len(eval_samples))\n",
    "\n",
    "    # init the trainer and 🚀\n",
    "    trainer = Trainer(\n",
    "        TrainerArgs(\n",
    "            restore_path=None,  # xtts checkpoint is restored via xtts_checkpoint key so no need of restore it using Trainer restore_path parameter\n",
    "            skip_train_epoch=False,\n",
    "            start_with_eval=START_WITH_EVAL,\n",
    "            grad_accum_steps=GRAD_ACUMM_STEPS,\n",
    "        ),\n",
    "        config,\n",
    "        output_path=OUT_PATH,\n",
    "        model=model,\n",
    "        train_samples=train_samples,\n",
    "        eval_samples=eval_samples,\n",
    "    )\n",
    "\n",
    "    trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "959b2c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549431 61052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      " > Training Environment:\n",
      " | > Backend: Torch\n",
      " | > Mixed precision: False\n",
      " | > Precision: float32\n",
      " | > Current device: 0\n",
      " | > Num. of GPUs: 1\n",
      " | > Num. of CPUs: 80\n",
      " | > Num. of Torch Threads: 1\n",
      " | > Torch seed: 1\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n",
      " > Start Tensorboard: tensorboard --logdir=/home/j-i13a103/tts/run/training/GPT_XTTS_v2.0_MyDataset_FT-July-28-2025_07+54AM-0000000\n",
      "\n",
      " > Model has 518442047 parameters\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/999\u001b[0m\n",
      " --> /home/j-i13a103/tts/run/training/GPT_XTTS_v2.0_MyDataset_FT-July-28-2025_07+54AM-0000000\n",
      " ! Run is removed from /home/j-i13a103/tts/run/training/GPT_XTTS_v2.0_MyDataset_FT-July-28-2025_07+54AM-0000000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/j-i13a103/.local/lib/python3.12/site-packages/trainer/trainer.py\", line 725, in get_eval_dataloader\n",
      "    return model.get_eval_data_loader(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/j-i13a103/.local/lib/python3.12/site-packages/trainer/model.py\", line 134, in get_eval_data_loader\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/j-i13a103/.local/lib/python3.12/site-packages/trainer/trainer.py\", line 1550, in fit\n",
      "    self._fit()\n",
      "  File \"/home/j-i13a103/.local/lib/python3.12/site-packages/trainer/trainer.py\", line 1504, in _fit\n",
      "    self.eval_epoch()\n",
      "  File \"/home/j-i13a103/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/j-i13a103/.local/lib/python3.12/site-packages/trainer/trainer.py\", line 1365, in eval_epoch\n",
      "    self.get_eval_dataloader(\n",
      "  File \"/home/j-i13a103/.local/lib/python3.12/site-packages/trainer/trainer.py\", line 734, in get_eval_dataloader\n",
      "    return self._get_loader(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/j-i13a103/.local/lib/python3.12/site-packages/trainer/trainer.py\", line 660, in _get_loader\n",
      "    assert len(loader) > 0, (\n",
      "           ^^^^^^^^^^^^^^^\n",
      "AssertionError:  ❗ len(DataLoader) returns 0. Make sure your dataset is not empty or len(dataset) > 0. \n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3557: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (coqui)",
   "language": "python",
   "name": "llm310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
