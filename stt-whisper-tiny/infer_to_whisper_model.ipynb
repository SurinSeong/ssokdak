{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a389cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "import wave\n",
    "\n",
    "# 추가\n",
    "import numpy as np\n",
    "from faster_whisper import WhisperModel\n",
    "import speech_recognition as sr\n",
    "import noiseruduce as nr\n",
    "import webrtcvad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd9cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Audio_record:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        요청 받았을 때 오디오를 스트리밍하여 원하는 만큼 녹음해서 디노이즈\n",
    "        \"\"\"\n",
    "        # 기본 선언\n",
    "        self.sample_rate = 16000\n",
    "        self.chunk_duration_ms = 30    # 청크의 길이를 ms 단위로 지정 (VAD 로직에서 필요함.)\n",
    "        self.vad_sec = 1    # n초 이상이 말이 없을 경우 녹음을 중지한다.\n",
    "        self.chunk_size = int(self.sample_rate * self.chunk_duration_ms / 1000)    # 청크 크기를 샘플 단위로 계산\n",
    "        self.recognizer = sr.Recognizer()\n",
    "        self.microphone = sr.Microphone(sample_rate=self.sample_rate, chunk_size=self.chunk_size)\n",
    "        self.buffer = []\n",
    "        self.recording = False\n",
    "\n",
    "        # VAD 초기화\n",
    "        self.vad = webrtcvad.Vad(1)    # 민감도 설정\n",
    "\n",
    "        # 주변 소음 조정\n",
    "        self.adjust_noise()\n",
    "\n",
    "        print(\"Audio_record 초기화 성공\")\n",
    "\n",
    "    def adjust_noise(self):\n",
    "        \"\"\"\n",
    "        주변 소음 조정하기\n",
    "        \"\"\"\n",
    "        with self.microphone as source:\n",
    "            print('주변 소음에 맞게 조정 중...')\n",
    "            self.recognizer.adjust_for_ambient_noise(source)\n",
    "            self.recognizer.energy_threshold += 100\n",
    "\n",
    "    def record_start(self):\n",
    "        '''녹음이 시작되는 함수'''\n",
    "        if self.recording == False:\n",
    "            self.record_thread = threading.Thread(target=self._record_start)\n",
    "            self.record_thread.start()\n",
    "    \n",
    "    def _record_start(self):\n",
    "        '''VAD 감지 조건으로 녹음이 게속되는 내부 함수'''\n",
    "        self.recording = True\n",
    "        self.buffer = []\n",
    "        no_voice_target_cnt = (self.vad_sec*1000) # 녹음 목표 초를 ms로 변환\n",
    "        no_voice_cnt = 0 # 위 변수와 비교할 cnt 설정\n",
    "        with self.microphone as source:\n",
    "            while self.recording:\n",
    "                chunk = source.stream.read(self.chunk_size)\n",
    "                self.buffer.append(chunk)\n",
    "                if self._vad(chunk, self.sample_rate):\n",
    "                    no_voice_cnt = 0\n",
    "                else:\n",
    "                    no_voice_cnt += self.chunk_duration_ms\n",
    "                # vad가 일정 시간 감지 안되면 녹음 중지\n",
    "                if no_voice_cnt >= no_voice_target_cnt:\n",
    "                    self.recording = False\n",
    "\n",
    "    def _vad(self, chunk, sample_rate):\n",
    "        '''주어진 청크가 음성인지 여부를 반환하는 함수'''\n",
    "        # 청크가 int16 형식인지 확인\n",
    "        if isinstance(chunk, bytes):\n",
    "            chunk = np.frombuffer(chunk, dtype=np.int16)\n",
    "        # 청크가 10ms, 20ms, 30ms 길이인지 확인\n",
    "        if len(chunk) != self.chunk_size:\n",
    "            raise ValueError(\"Chunk size must be exactly 10ms, 20ms, or 30ms\")\n",
    "        return self.vad.is_speech(chunk.tobytes(), sample_rate)\n",
    "        \n",
    "    def record_stop(self, denoise_value):\n",
    "        '''녹음이 종료되고 디노이징 과정을 거치는 함수'''\n",
    "        # thread 종료하고, 끝날 때 까지 join으로 대기\n",
    "        self.recording = False\n",
    "        self.record_thread.join()\n",
    "        # 버퍼를 하나의 오디오 데이터로 결합\n",
    "        audio_data = np.frombuffer(b''.join(self.buffer), dtype=np.int16)\n",
    "        sample_rate = self.microphone.SAMPLE_RATE\n",
    "        return self._denoise_process(audio_data, sample_rate, denoise_value)\n",
    "\n",
    "    \n",
    "    def load_wav(self, path, denoise_value):\n",
    "        '''wav파일을 불러와 디노이징 과정을 거치는 함수'''\n",
    "        buffer = []\n",
    "        with wave.open(path, 'rb') as wf:\n",
    "            chunk_size = self.chunk_size\n",
    "            data = wf.readframes(chunk_size)\n",
    "            while data:\n",
    "                buffer.append(data)\n",
    "                data = wf.readframes(chunk_size)\n",
    "\n",
    "        audio_data = np.frombuffer(b''.join(buffer), dtype=np.int16)\n",
    "        sample_rate = wf.getframerate()\n",
    "        return self._denoise_process(audio_data, sample_rate, denoise_value)\n",
    "\n",
    "    \n",
    "    def _denoise_process(self, audio_data, sample_rate, denoise_value):\n",
    "        '''\n",
    "        오디오를 받아 디노이징을 적용하고, 원본과 디노이즈값둘 둘 다 저장하고 반환한다.\n",
    "        \n",
    "        audio_data : int16 np 형식 오디오 데이터. chunk를 append하여 만들어진 buffer를 다음과 같이 처리한 예시) np.frombuffer(b''.join(self.buffer), dtype=np.int16)\n",
    "        sample_rate : 샘플 레이트 입력\n",
    "        denoise_value : 디노이즈 적용값 설정\n",
    "        \n",
    "        return: {'audio_denoise': audio_denoise, 'audio_noise': audio_noise, 'sample_rate': sample_rate}\n",
    "        '''\n",
    "        # 1. 노이즈 감소 처리\n",
    "        denoise = nr.reduce_noise(y=audio_data, sr=sample_rate, prop_decrease=denoise_value)\n",
    "        buffer_denoise = [denoise.tobytes()] # 데이터를 다시 버퍼로 변환\n",
    "        # 2. 노이즈 감소 없이\n",
    "        noise = nr.reduce_noise(y=audio_data, sr=sample_rate, prop_decrease=0.0)\n",
    "        buffer_noise = [noise.tobytes()] # 데이터를 다시 버퍼로 변환\n",
    "        \n",
    "        # 1. 노이즈 감소 파일 저장\n",
    "        self._save_buffer_to_wav(buffer_denoise, self.microphone.SAMPLE_RATE, self.microphone.SAMPLE_WIDTH, 'input_denoise.wav')\n",
    "        # 2. 노이즈 감소 없는 파일 저장\n",
    "        self._save_buffer_to_wav(buffer_noise, self.microphone.SAMPLE_RATE, self.microphone.SAMPLE_WIDTH, 'input_noise.wav')\n",
    "        \n",
    "        # 오디오 소스 파일로 return\n",
    "        audio_denoise = self._buffer_to_numpy(buffer_denoise, self.microphone.SAMPLE_RATE)\n",
    "        audio_noise = self._buffer_to_numpy(buffer_noise, self.microphone.SAMPLE_RATE)\n",
    "\n",
    "        return {'audio_denoise':audio_denoise, 'audio_noise':audio_noise, 'sample_rate':self.microphone.SAMPLE_RATE}\n",
    "\n",
    "\n",
    "    def _buffer_to_numpy(self, buffer, sample_rate):\n",
    "        '''buffer를 입력하면 whisper에서 추론 가능한 입력 형태의 오디오로 반환'''\n",
    "        audio_data = np.frombuffer(b''.join(buffer), dtype=np.int16)\n",
    "        audio_data = audio_data.astype(np.float32) / 32768.0  # Convert to float32        \n",
    "        return audio_data\n",
    "        \n",
    "\n",
    "    def _save_buffer_to_wav(self, buffer, sample_rate, sample_width, filename):\n",
    "        with wave.open(filename, 'wb') as wf:\n",
    "            wf.setnchannels(1)  # 모노\n",
    "            wf.setsampwidth(sample_width)\n",
    "            wf.setframerate(sample_rate)\n",
    "            wf.writeframes(b''.join(buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b60c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cumtom_faster_whisper:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        최대 4배 빠른 faster whisper를 사용하여 cpu로 저장된 wav파일에 STT 수행\n",
    "        '''\n",
    "        # 환경 설정(Window 아나콘다 환경에서 아래 코드 실행 안하면 에러남)\n",
    "        try: os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"true\"\n",
    "        except Exception as e: print(f'os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"true\" 실행해서 발생한 에러. 하지만 무시하고 진행: {e}')\n",
    "\n",
    "        try: os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "        except Exception as e: print(f'os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" 실행해서 발생한 에러. 하지만 무시하고 진행: {e}')\n",
    "        print('Cumtom_faster_whisper 초기화 성공')\n",
    "\n",
    "    def set_model(self, model_name):\n",
    "        '''\n",
    "        모델 설정\n",
    "        '''\n",
    "        model_list = ['tiny', 'tiny.en', 'base', 'base.en', 'small', 'small.en', 'medium', 'medium.en', 'large-v1', 'large-v2', 'large-v3', 'large']\n",
    "        if not model_name in model_list:\n",
    "            model_name = 'base'\n",
    "            print('모델 이름 잘못됨. base로 설정. 아래 모델 중 한가지 선택')\n",
    "            print(model_list)\n",
    "        self.model = WhisperModel(model_name, device=\"cpu\", compute_type=\"int8\")\n",
    "        return model_name\n",
    "\n",
    "    def run(self, audio, language=None):\n",
    "        '''\n",
    "        저장된 tmp.wav를 불러와서 STT 추론 수행\n",
    "\n",
    "        audio : wav파일의 경로 or numpy로 변환된 오디오 파일 소스\n",
    "        language : ko, en 등 언어 선택 가능. 선택하지 않으면 언어 분류 모델 내부적으로 수행함\n",
    "        '''\n",
    "        start = time.time()\n",
    "        # 추론\n",
    "\n",
    "        segments, info = self.model.transcribe(audio, beam_size=5, word_timestamps=True, language=language)\n",
    "        # 결과 후처리\n",
    "        dic_list = []\n",
    "        for segment in segments:\n",
    "            if segment.no_speech_prob > 0.6: continue # 말을 안했을 확률이 크다고 감지되면 무시\n",
    "            for word in segment.words:\n",
    "                _word = word.word\n",
    "                _start = round(word.start, 2)\n",
    "                _end = round(word.end, 2)\n",
    "                dic_list.append([_word, _start, _end])\n",
    "        # 시간 계산\n",
    "        self.spent_time = round(time.time()-start, 2)\n",
    "        \n",
    "        # 텍스트 추출\n",
    "        result_txt = self._make_txt(dic_list)\n",
    "        print(result_txt)\n",
    "        return dic_list, result_txt, self.spent_time\n",
    "\n",
    "    def _make_txt(self, dic_list):\n",
    "        '''\n",
    "        [word, start, end]에서 word만 추출하여 txt로 반환\n",
    "        '''\n",
    "        result_txt = ''\n",
    "        for dic in dic_list:\n",
    "            txt = dic[0]\n",
    "            result_txt = f'{result_txt}{txt}'\n",
    "        return result_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328747f5",
   "metadata": {},
   "source": [
    "# 깃헙 따라하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a560a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "240618 실시간 STT 테스트\n",
    "\n",
    "[문제점]\n",
    "- tmp.wav를 저장해서 불러오는 방식이라 그 과정에서 계속 알 수 없는 에러가 발생(읽는 타이밍이 꼬이는듯)\n",
    "- 마지막 단어를 배출하지 못하는 특성 있음. 그리고 10초가 지나면 잊혀짐\n",
    "- wav파일을 넣었을 때랑, tmp.wav를 실시간으로 불러왔을 때랑 추론 결과가 다른 것 같음. 전자가 성능이 더 좋은 것 같음\n",
    "'''\n",
    "\n",
    "\n",
    "import pyaudio\n",
    "import wave\n",
    "import os\n",
    "import threading\n",
    "import shutil\n",
    "\n",
    "class Audio_streaming:\n",
    "    def __init__(self, sr=32000, save_sec=10):\n",
    "        '''\n",
    "        마이크를 실시간으로 입력받아 wav파이롤 저장해주는 기능\n",
    "\n",
    "        sr : 샘플레이트\n",
    "        save_sec : 순차적으로 저장될 wav파일의 길이(초)\n",
    "        overlap_sec : 오버랩될 길이(초)\n",
    "        save_path : 저장 경로\n",
    "        '''\n",
    "        self.save_sec = save_sec\n",
    "        self.chunk = sr # chunk 1개는 sr레이트랑 동일하다. 1초라는 의미\n",
    "        self.buffer = [] # chunk를 쌓아두는 리스트\n",
    "        self.lost_secs = 0 # buffer에서 실시간으로 버리는 chunk 개수 기록\n",
    "        self.streaming = True # False가 되면 multi-thread들이 종료된다\n",
    "\n",
    "        # 오디오 관련 선언\n",
    "        self.sr = sr\n",
    "        self.audio = pyaudio.PyAudio()\n",
    "        self.format = pyaudio.paInt16\n",
    "        self.channels = 1\n",
    "        self.stream = self.audio.open(format=self.format, channels=self.channels, rate=self.sr, input=True, frames_per_buffer=self.chunk)\n",
    "\n",
    "    def run(self):\n",
    "        print('오디오 스트리밍, tmp.wav 저장 시작')\n",
    "        threading.Thread(target=self._run).start()\n",
    "\n",
    "    def stop(self):\n",
    "        '''\n",
    "        스트리밍 중지\n",
    "        '''\n",
    "        self.streaming = False\n",
    "\n",
    "\n",
    "    def save_buffer(self):\n",
    "        '''\n",
    "        buffer에 있는 최근의 n초를 저장한다. 계산을 쉽게 하기 위하여 1개의 chunk는 무조건 1초로 한다. 그래서 n초는 buffer에서 n개의 원소를 뜻한다.\n",
    "        '''\n",
    "        # buffer가 n초 이상을 넘어가지 않게 관리한다.\n",
    "        if len(self.buffer) > self.save_sec:\n",
    "            self.lost_secs += len(self.buffer) - self.save_sec\n",
    "            self.buffer = self.buffer[-self.save_sec:]\n",
    "        # 최근의 n초를 저장한다\n",
    "        self._frames_to_wav(self.buffer)\n",
    "\n",
    "        # 버린 누적 chunk와 방금 저장한 buffer의 길이를 반환\n",
    "        return self.lost_secs, len(self.buffer)\n",
    "\n",
    "\n",
    "    def _run(self):\n",
    "        '''\n",
    "        스트리밍하여 buffer에 지속적으로 음성 chunk를 추가만 하는 쓰레드\n",
    "        '''\n",
    "        while self.streaming:\n",
    "            one_chunk = self.stream.read(self.chunk)\n",
    "            self.buffer.append(one_chunk)\n",
    "        print('\\n스트리밍 종료')\n",
    "\n",
    "    def _frames_to_wav(self, frames):\n",
    "        '''\n",
    "        입력된 buffer안의 원소들을 join하여 wav로 저장\n",
    "        '''\n",
    "        # 기존 파일 삭제\n",
    "        wav_name = 'tmp.wav'\n",
    "        if os.path.exists(wav_name):\n",
    "            # 다른 프로세스에서 파일을 사용중이라는 에러가 발생하여 try문으로 무한 시도하게 개조\n",
    "            while True:\n",
    "                try: os.remove(wav_name); break\n",
    "                except: continue\n",
    "        # 새로운 wav 저장\n",
    "        with wave.open(wav_name, 'wb') as wf:\n",
    "            wf.setnchannels(self.channels)\n",
    "            wf.setsampwidth(self.audio.get_sample_size(self.format))\n",
    "            wf.setframerate(self.sr)\n",
    "            wf.writeframes(b''.join(frames))\n",
    "\n",
    "\n",
    "import os\n",
    "from faster_whisper import WhisperModel\n",
    "import time\n",
    "\n",
    "class STT_faster_whisper:\n",
    "    def __init__(self, model_size):\n",
    "        '''\n",
    "        최대 4배 빠른 faster whisper를 사용하여 cpu로 저장된 wav파일에 STT 수행\n",
    "        \n",
    "        model_size : tiny, tiny.en, base, base.en, small, small.en, medium, medium.en, large-v1, large-v2, large-v3, or large\n",
    "        read_path : wav가 저장되어있는 폴더 경로\n",
    "        '''\n",
    "        # 환경 설정(Window 아나콘다 환경에서 아래 코드 실행 안하면 에러남)\n",
    "        try: os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"true\"\n",
    "        except Exception as e: print(f'os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"true\" 실행해서 발생한 에러. 하지만 무시하고 진행: {e}')\n",
    "\n",
    "        try: os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "        except Exception as e: print(f'os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" 실행해서 발생한 에러. 하지만 무시하고 진행: {e}')\n",
    "\n",
    "        # 모델 선언\n",
    "        self.model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "    \n",
    "    def run(self, wav_path, last_del=False):\n",
    "        '''\n",
    "        wav 경로를 입력하면 txt로 변경해주고, 각 단어에 대한 time stamp를 반환함\n",
    "\n",
    "        wav_path : STT를 수행할 wav파일 full 경로\n",
    "        last_del : STT된 마지막 word를 지울지. 마지막 word는 끊겼을 수 있다고 가정하기 때문\n",
    "        '''\n",
    "        start = time.time()\n",
    "        # 인퍼런스\n",
    "        segments, info = self.model.transcribe(wav_path, beam_size=5, word_timestamps=True, language='ko')\n",
    "\n",
    "        # 결과 후처리\n",
    "        dic_list = []\n",
    "        for segment in segments:\n",
    "            if segment.no_speech_prob > 0.6: continue # 말을 안했을 확률이 크다고 감지되면 무시\n",
    "            for word in segment.words:\n",
    "                _word = word.word\n",
    "                _start = round(word.start, 2)\n",
    "                _end = round(word.end, 2)\n",
    "                dic_list.append([_word, _start, _end])\n",
    "        self.time = round(time.time()-start, 2)\n",
    "        # 마지막 word 삭제 옵션 적용\n",
    "        if last_del == True and len(dic_list) > 0:\n",
    "            del dic_list[-1]\n",
    "        return dic_list\n",
    "    \n",
    "import time\n",
    "\n",
    "class Realtime_stt:\n",
    "    def __init__(self, model_size):\n",
    "        '''\n",
    "        실시간으로 마이크에서 음성을 저장하는 동시에, 꺼내와서 STT해주는 모듈\n",
    "        핵심 기술:\n",
    "        - 실시간으로 overlap 하여 저장\n",
    "        - 선입선출로 음성을 가져와 STT 추론\n",
    "        - 추론된 결과를 바탕으로 time stamp 기준으로 통합\n",
    "        - 추론된 결과 실시간 제공\n",
    "        '''\n",
    "        self.model_size = model_size\n",
    "        self.streaming = True\n",
    "\n",
    "        self.total_dic_list = [] # 모든 텍스트 히스토리를 저장함\n",
    "        self.stt_model = STT_faster_whisper(model_size)\n",
    "        self.audio_streaming = Audio_streaming()\n",
    "        self.txt_log = ''\n",
    "        \n",
    "    def run(self):\n",
    "        self.audio_streaming.run()\n",
    "        threading.Thread(target=self._run).start()\n",
    "        \n",
    "    def _run(self):\n",
    "        while self.streaming:\n",
    "            # 최근 오디오(최대10초) 저장\n",
    "            lost_secs, buffer_len = self.audio_streaming.save_buffer()\n",
    "            # 저장된 오디오 STT(마지막 word 제외)\n",
    "            new_dic_list = self.stt_model.run('tmp.wav', last_del=True)\n",
    "            updated_dic_list = self._time_update(new_dic_list, lost_secs)\n",
    "            # total_dic_list에 새로운 텍스트 중복 제거 병합\n",
    "            self.total_dic_list = self._murge_dic_list(self.total_dic_list, updated_dic_list)\n",
    "            # 결과 출력\n",
    "            result_txt = self._get_txt_from_dic_list(self.total_dic_list)\n",
    "            self._txt_out(result_txt)\n",
    "\n",
    "        \n",
    "    def _time_update(self, new_dic_list, lost_secs):\n",
    "        '''\n",
    "        new_dic_list의 start, end 값들을 실제 처럼 업데이트\n",
    "        '''\n",
    "        updated_dic_list = []\n",
    "        for dic in new_dic_list:\n",
    "            dic[1] += lost_secs\n",
    "            dic[2] += lost_secs\n",
    "            updated_dic_list.append(dic)\n",
    "        return updated_dic_list\n",
    "            \n",
    "    def _murge_dic_list(self, total_dic_list, new_dic_list):\n",
    "        '''\n",
    "        time stamp를 확인하여 두 stt결과를 중복 제거하여 병합\n",
    "        '''\n",
    "        # 마지막 단어가 끝나는 시점 가져오기\n",
    "        if len(total_dic_list) > 0:\n",
    "            last_end = total_dic_list[-1][2]\n",
    "        else:\n",
    "            last_end = 0\n",
    "        # 새로운 리스트 병합하기\n",
    "        for dic in new_dic_list:\n",
    "            # 추가 조건 확인\n",
    "            if dic[1] >= last_end: # dic 데이터 예시: [word, start, end]\n",
    "                total_dic_list.append(dic)\n",
    "            else:\n",
    "                continue\n",
    "        return total_dic_list\n",
    "            \n",
    "    def _get_txt_from_dic_list(self, dic_list):\n",
    "        '''\n",
    "        dic_list에서 txt만 뽑아서 반환\n",
    "        '''\n",
    "        txt = ''\n",
    "        for dic in dic_list:\n",
    "            new_txt = dic[0]\n",
    "            txt = f'{txt}{new_txt}'\n",
    "        return txt\n",
    "    \n",
    "    def _txt_out(self, txt):\n",
    "        '''\n",
    "        전체 텍스트를 출력 요청하면, 지금까지 출력된 텍스트를 제외하고 출력\n",
    "        '''\n",
    "        new_txt = txt[len(self.txt_log):]\n",
    "        if len(new_txt) > 0:\n",
    "            print(new_txt, end='')\n",
    "        self.txt_log = txt\n",
    "\n",
    "    def stop(self):\n",
    "        '''\n",
    "        멀티쓰레드로 구동되는 스트리밍 로직을 중지\n",
    "        '''\n",
    "        print('프로세스 중지 중...')\n",
    "        self.audio_streaming.stop()\n",
    "        self.streaming = False\n",
    "\n",
    "realtime_stt = Realtime_stt(model_size='base')\n",
    "realtime_stt.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stt)",
   "language": "python",
   "name": "stt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
